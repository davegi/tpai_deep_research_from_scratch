{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da8d2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: Import core framework classes (deferred until implemented)\n",
    "# Ensure local `src` is on sys.path so notebook kernels can import the package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "src = Path(\"../src\").resolve()\n",
    "if str(src) not in sys.path:\n",
    "    sys.path.insert(0, str(src))\n",
    "\n",
    "from research_agent_framework.llm.client import MockLLM, LLMConfig\n",
    "from research_agent_framework.adapters.search.mock_search import MockSearchAdapter\n",
    "\n",
    "# Expose demo imports for following cells\n",
    "__all__ = [\"MockLLM\", \"LLMConfig\", \"MockSearchAdapter\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e0c3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: Bootstrap environment\n",
    "# Use the project's bootstrap to configure logging and settings for demos\n",
    "from research_agent_framework.bootstrap import bootstrap\n",
    "bootstrap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b19d1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: sys.path fix for local imports (kept minimal)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "src = Path(\"../src\").resolve()\n",
    "if str(src) not in sys.path:\n",
    "    sys.path.insert(0, str(src))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99578f6",
   "metadata": {},
   "source": [
    "# Consolidated Research Agent Demo\n",
    "\n",
    "This notebook demonstrates the use of the `research_agent_framework` package. Each section will be updated as new features are implemented.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32cd41",
   "metadata": {},
   "source": [
    "# Consolidated Research Agent Demo Notebook\n",
    "\n",
    "This notebook demonstrates the use of the `research_agent_framework` package and its components. Each section is marked for traceability to the corresponding PRD task.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ca0d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: Bootstrap environment (safe to call multiple times in a notebook)\n",
    "from research_agent_framework.bootstrap import bootstrap\n",
    "bootstrap(force=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b916ee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">scope</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Scope</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">topic</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Coffee Shops'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Find coffee shops in SF'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">constraints</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'no paid sources'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mscope\u001b[0m=\u001b[1;35mScope\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtopic\u001b[0m=\u001b[32m'Coffee Shops'\u001b[0m, \u001b[33mdescription\u001b[0m=\u001b[32m'Find coffee shops in SF'\u001b[0m, \u001b[33mconstraints\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'no paid sources'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">task</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResearchTask</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'t-001'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">query</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'best coffee in soma'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">context</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">notes</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mtask\u001b[0m=\u001b[1;35mResearchTask\u001b[0m\u001b[1m(\u001b[0m\u001b[33mid\u001b[0m=\u001b[32m't-001'\u001b[0m, \u001b[33mquery\u001b[0m=\u001b[32m'best coffee in soma'\u001b[0m, \u001b[33mcontext\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mnotes\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">eval_result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EvalResult</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">task_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'t-001'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">success</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95</span>, <span style=\"color: #808000; text-decoration-color: #808000\">feedback</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Looks good'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">details</span>=<span style=\"font-weight: bold\">{})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33meval_result\u001b[0m=\u001b[1;35mEvalResult\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtask_id\u001b[0m=\u001b[32m't-001'\u001b[0m, \u001b[33msuccess\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.95\u001b[0m, \u001b[33mfeedback\u001b[0m=\u001b[32m'Looks good'\u001b[0m, \u001b[33mdetails\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">serp</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SerpResult</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">title</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Cafe Example'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HttpUrl</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'https://example.com/'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">snippet</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Great coffee'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">raw</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mserp\u001b[0m=\u001b[1;35mSerpResult\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtitle\u001b[0m=\u001b[32m'Cafe Example'\u001b[0m, \u001b[33murl\u001b[0m=\u001b[1;35mHttpUrl\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'https://example.com/'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33msnippet\u001b[0m=\u001b[32m'Great coffee'\u001b[0m, \u001b[33mraw\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK-2.3: models demo\n",
    "from research_agent_framework.config import Settings, get_settings\n",
    "from research_agent_framework.models import Scope, ResearchTask, EvalResult, SerpResult\n",
    "from pydantic import TypeAdapter, HttpUrl\n",
    "from assertpy import assert_that\n",
    "\n",
    "# Construct model instances\n",
    "scope = Scope(topic='Coffee Shops', description='Find coffee shops in SF', constraints=['no paid sources'])\n",
    "task = ResearchTask(id='t-001', query='best coffee in soma')\n",
    "eval_result = EvalResult(task_id=task.id, success=True, score=0.95, feedback='Looks good')\n",
    "# Use TypeAdapter to validate/construct an HttpUrl (pydantic v2)\n",
    "url_adapter = TypeAdapter(HttpUrl)\n",
    "validated_url = url_adapter.validate_python('https://example.com')\n",
    "serp = SerpResult(title='Cafe Example', url=validated_url, snippet='Great coffee', raw={'id': 1})\n",
    "\n",
    "# Example asserts using assertpy\n",
    "assert_that(scope.topic).is_equal_to('Coffee Shops')\n",
    "assert_that(scope.constraints).contains('no paid sources')\n",
    "assert_that(task.id).is_equal_to('t-001')\n",
    "assert_that(eval_result.success).is_true()\n",
    "assert_that(serp.url).is_instance_of(HttpUrl)\n",
    "\n",
    "from rich.console import Console\n",
    "from typing import cast\n",
    "console = get_settings().console\n",
    "assert_that(console).is_not_none()\n",
    "console = cast(Console, console)\n",
    "console.print(f\"{scope=}\")\n",
    "console.print(f\"{task=}\")\n",
    "console.print(f\"{eval_result=}\")\n",
    "console.print(f\"{serp=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf3ad70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested SerpResult (simple):\n",
      "{'title': 'Nested Cafe', 'url': HttpUrl('https://example.com/nested'), 'snippet': 'A nested example', 'raw': {'id': 'nested-1'}}\n",
      "clarify_with_user_instructions.j2 output:\n",
      " These are the messages that have been exchanged so far from the user asking for the report:\n",
      "<Messages>\n",
      "User: What are the best coffee shops in SF?\n",
      "</Messages>\n",
      "\n",
      "Today's date is 2025-09-05.\n",
      "\n",
      "Assess whether you need to ask a clarifying question, or if the user has already provided enough information for you to start research.\n",
      "IMPORTANT: If you can see in the messages history that you have already asked a clarifying question, you almost always do not need to ask another one. Only ask another question if ABSOLUTELY NECESSARY.\n",
      "\n",
      "If there are acronyms, abbreviations, or unknown terms, ask the user to clarify.\n",
      "If you need to ask a question, follow these guidelines:\n",
      "- Be concise while gathering all necessary information\n",
      "- Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner.\n",
      "- Use bullet points or numbered lists if appropriate for clarity. Make sure that this uses markdown formatting and will be rendered correctly if the string output is passed to a markdown renderer.\n",
      "- Don't ask for unnecessary information, or information that the user has already provided. If you can see that the user has already provided the information, do not ask for it again.\n",
      "\n",
      "Respond in valid JSON format with these exact keys:\n",
      "\"need_clarification\": boolean,\n",
      "\"question\": \"<question to ask the user to clarify the report scope>\",\n",
      "\"verification\": \"<verification message that we will start research>\"\n",
      "\n",
      "If you need to ask a clarifying question, return:\n",
      "\"need_clarification\": true,\n",
      "\"question\": \"<your clarifying question>\",\n",
      "\"verification\": \"\"\n",
      "\n",
      "If you do not need to ask a clarifying question, return:\n",
      "\"need_clarification\": false,\n",
      "\"question\": \"\",\n",
      "\"verification\": \"<acknowledgement message that you will now start research based on the provided information>\"\n",
      "\n",
      "For the verification message when no clarification is needed:\n",
      "- Acknowledge that you have sufficient information to proceed\n",
      "- Briefly summarize the key aspects of what you understand from their request\n",
      "- Confirm that you will now begin the research process\n",
      "- Keep the message concise and professional\n",
      "research_agent_prompt.j2 output:\n",
      " You are a research assistant conducting research on the user's input topic. For context, today's date is 2025-09-05.\n",
      "\n",
      "<Task>\n",
      "Your job is to use tools to gather information about the user's input topic.\n",
      "You can use any of the tools provided to you to find resources that can help answer the research question. You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
      "</Task>\n",
      "\n",
      "<Available Tools>\n",
      "You have access to two main tools:\n",
      "1. **tavily_search**: For conducting web searches to gather information\n",
      "2. **think_tool**: For reflection and strategic planning during research\n",
      "\n",
      "**CRITICAL: Use think_tool after each search to reflect on results and plan next steps**\n",
      "</Available Tools>\n",
      "\n",
      "<Instructions>\n",
      "Think like a human researcher with limited time. Follow these steps:\n",
      "\n",
      "1. **Read the question carefully** - What specific information does the user need?\n",
      "2. **Start with broader searches** - Use broad, comprehensive queries first\n",
      "3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
      "4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
      "5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
      "</Instructions>\n",
      "\n",
      "<Hard Limits>\n",
      "**Tool Call Budgets** (Prevent excessive searching):\n",
      "- **Simple queries**: Use 2-3 search tool calls maximum\n",
      "- **Complex queries**: Use up to 5 search tool calls maximum\n",
      "- **Always stop**: After 5 search tool calls if you cannot find the right sources\n",
      "\n",
      "**Stop Immediately When**:\n",
      "- You can answer the user's question comprehensively\n",
      "- You have 3+ relevant examples/sources for the question\n",
      "- Your last 2 searches returned similar information\n",
      "</Hard Limits>\n",
      "\n",
      "<Show Your Thinking>\n",
      "After each search tool call, use think_tool to analyze the results:\n",
      "- What key information did I find?\n",
      "- What's missing?\n",
      "- Do I have enough to answer the question comprehensively?\n",
      "- Should I search more or provide my answer?\n"
     ]
    }
   ],
   "source": [
    "# TASK-3: renderer example\n",
    "from pydantic import TypeAdapter, HttpUrl\n",
    "# Import nested models with graceful fallback if not present in the import path\n",
    "try:\n",
    "    from research_agent_framework.models import (\n",
    "        SerpResult, Location, Address, Coordinates, Rating, PriceLevel, ProviderMeta\n",
    "    )\n",
    "    _has_nested_models = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from research_agent_framework.models import SerpResult\n",
    "        _has_nested_models = False\n",
    "    except Exception:\n",
    "        SerpResult = None\n",
    "        _has_nested_models = False\n",
    "\n",
    "if SerpResult is None:\n",
    "    print('research_agent_framework.models not available in this environment; skipping renderer example.')\n",
    "else:\n",
    "    if _has_nested_models:\n",
    "        # Build nested models (full demo)\n",
    "        coords = Coordinates(lat=37.7749, lon=-122.4194)\n",
    "        addr = Address(street='123 Example St', city='San Francisco', region='CA', postal_code='94103', country='US')\n",
    "        loc = Location(name='Cafe Nested', address=addr, coords=coords)\n",
    "        rating = Rating(score=4.6, count=128)\n",
    "        provider = ProviderMeta(provider='mock', id=42, raw={'provider_field': 'value'})\n",
    "        url_adapter = TypeAdapter(HttpUrl)\n",
    "        u = url_adapter.validate_python('https://example.com/nested')\n",
    "        s = SerpResult(title='Nested Cafe', url=u, snippet='A nested example', raw={'id': 'nested-1'}, location=loc, rating=rating, price_level=PriceLevel.MODERATE, categories=['cafe','coffee'], provider_meta=provider)\n",
    "        print('Nested SerpResult:')\n",
    "        print(s.model_dump())\n",
    "    else:\n",
    "        # Simple SerpResult demo without nested models (keeps compatibility)\n",
    "        url_adapter = TypeAdapter(HttpUrl)\n",
    "        u = url_adapter.validate_python('https://example.com/nested')\n",
    "        s = SerpResult(title='Nested Cafe', url=u, snippet='A nested example', raw={'id': 'nested-1'})\n",
    "        print('Nested SerpResult (simple):')\n",
    "        print(s.model_dump())\n",
    "\n",
    "from research_agent_framework.prompts import renderer\n",
    "\n",
    "# Render clarify_with_user_instructions template\n",
    "clarify_context = {\"messages\": \"User: What are the best coffee shops in SF?\", \"date\": \"2025-09-05\"}\n",
    "clarify_rendered = renderer.render_template(\"clarify_with_user_instructions.j2\", clarify_context)\n",
    "print(\"clarify_with_user_instructions.j2 output:\\n\", clarify_rendered)\n",
    "\n",
    "# Render research_agent_prompt template\n",
    "agent_context = {\"date\": \"2025-09-05\"}\n",
    "agent_rendered = renderer.render_template(\"research_agent_prompt.j2\", agent_context)\n",
    "print(\"research_agent_prompt.j2 output:\\n\", agent_rendered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d394de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockLLM output: mock response for: What are the best coffee shops in SF?\n",
      "MockSearchAdapter results:\n",
      "- Coffee Shop A (https://coffee.example.com/a) - Great coffee and friendly staff\n",
      "- Coffee Shop B (https://coffee.example.com/b) - Excellent pastries\n"
     ]
    }
   ],
   "source": [
    "# TASK-4.3: Import and use MockLLM and MockSearchAdapter for deterministic demo\n",
    "from research_agent_framework.llm.client import MockLLM, LLMConfig\n",
    "from research_agent_framework.adapters.search.mock_search import MockSearchAdapter\n",
    "import asyncio\n",
    "from typing import Any\n",
    "\n",
    "mock_config = LLMConfig(api_key=\"test\", model=\"mock-model\")\n",
    "mock_llm = MockLLM(mock_config)\n",
    "searcher = MockSearchAdapter()\n",
    "\n",
    "async def demo_llm_and_search():\n",
    "    prompt = \"What are the best coffee shops in SF?\"\n",
    "    llm_out = await mock_llm.generate(prompt)\n",
    "    results = await searcher.search(prompt)\n",
    "    print(\"MockLLM output:\", llm_out)\n",
    "    print(\"MockSearchAdapter results:\")\n",
    "    # Support both the legacy list return and the new SerpReply return\n",
    "    try:\n",
    "        from research_agent_framework.adapters.search.schema import SerpReply\n",
    "    except Exception:\n",
    "        SerpReply = None\n",
    "\n",
    "    if SerpReply is not None and isinstance(results, SerpReply):\n",
    "        items = results.results\n",
    "    elif isinstance(results, list):\n",
    "        items = results\n",
    "    else:\n",
    "        # Fallback: try attribute access but treat as iterable otherwise\n",
    "        items = getattr(results, 'results', results)\n",
    "\n",
    "    for r in items:\n",
    "        # Some adapters or earlier implementations may return tuples (title, url, snippet).\n",
    "        # Handle both tuple-like and object-like items safely to avoid attribute errors in notebooks.\n",
    "        if isinstance(r, tuple):\n",
    "            title = r[0] if len(r) > 0 else ''\n",
    "            url = r[1] if len(r) > 1 else ''\n",
    "            snippet = r[2] if len(r) > 2 else ''\n",
    "            print(f\"- {title} ({url}) - {snippet}\")\n",
    "        else:\n",
    "            # Assume object-like with attributes `title`, `url`, `snippet`\n",
    "            try:\n",
    "                title = getattr(r, 'title', '')\n",
    "                url = getattr(r, 'url', '')\n",
    "                snippet = getattr(r, 'snippet', '')\n",
    "                print(f\"- {title} ({url}) - {snippet}\")\n",
    "            except Exception as e:\n",
    "                print('Unexpected result item:', r, type(r), 'error:', e)\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    import nest_asyncio; nest_asyncio.apply()\n",
    "    asyncio.run(demo_llm_and_search())\n",
    "except RuntimeError:\n",
    "    asyncio.run(demo_llm_and_search())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "675b8b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockLLM property-based output: mock response for: Show me the best coffee shops in SF\n"
     ]
    }
   ],
   "source": [
    "# TASK-4A.3: Property-based example for MockLLM (kept as demonstration)\n",
    "from research_agent_framework.llm.client import LLMConfig, MockLLM\n",
    "from hypothesis import given, strategies as st\n",
    "import asyncio\n",
    "import pytest\n",
    "from assertpy import assert_that\n",
    "\n",
    "# Example: deterministic output for random prompt/config\n",
    "@pytest.mark.asyncio\n",
    "@given(\n",
    "    prompt=st.text(min_size=1, max_size=200),\n",
    "    api_key=st.text(min_size=1, max_size=20),\n",
    "    model=st.text(min_size=1, max_size=20),\n",
    ")\n",
    "async def demo_mockllm_property_valid(prompt, api_key, model):\n",
    "    config = LLMConfig(api_key=api_key, model=model)\n",
    "    client = MockLLM(config)\n",
    "    result = await client.generate(prompt)\n",
    "    assert_that(result).is_equal_to(f\"mock response for: {prompt}\")\n",
    "\n",
    "# Run a single example for demonstration\n",
    "async def run_demo():\n",
    "    config = LLMConfig(api_key=\"demo-key\", model=\"demo-model\")\n",
    "    client = MockLLM(config)\n",
    "    result = await client.generate(\"Show me the best coffee shops in SF\")\n",
    "    print(\"MockLLM property-based output:\", result)\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    import nest_asyncio; nest_asyncio.apply()\n",
    "    asyncio.run(run_demo())\n",
    "except RuntimeError:\n",
    "    asyncio.run(run_demo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c31c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned tasks:\n",
      "- 7d38cd4c Coffee Shops - constraint: no paid sources\n",
      "Run result: task_id='7d38cd4c' success=True score=0.61 feedback='mock response for: Coffee Shops - constraint: no paid sources' details={}\n"
     ]
    }
   ],
   "source": [
    "# TASK-5.1: Demonstrate ResearchAgent plan() and run() using MockLLM\n",
    "from research_agent_framework.agents.base import ResearchAgent\n",
    "from research_agent_framework.models import Scope\n",
    "\n",
    "agent = ResearchAgent(llm_client=MockLLM(LLMConfig(api_key='demo', model='demo')),\n",
    "                        search_adapter=MockSearchAdapter())\n",
    "\n",
    "scope = Scope(topic='Coffee Shops', description='Find notable coffee shops in SF', constraints=['no paid sources'])\n",
    "plans = agent.plan(scope)\n",
    "print('Planned tasks:')\n",
    "for t in plans:\n",
    "    print('-', t.id, t.query)\n",
    "\n",
    "import asyncio\n",
    "async def run_first():\n",
    "    res = await agent.run(plans[0])\n",
    "    print('Run result:', res)\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    import nest_asyncio; nest_asyncio.apply()\n",
    "    asyncio.run(run_first())\n",
    "except RuntimeError:\n",
    "    asyncio.run(run_first())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f343e",
   "metadata": {},
   "source": [
    "# Notebook Status\n",
    "\n",
    "This notebook contains runnable demos that reflect the current test suite and mock implementations in `src/research_agent_framework`.\n",
    "\n",
    "Sections included:\n",
    "\n",
    "- Models demonstration\n",
    "- Prompt renderer example\n",
    "- MockLLM + MockSearchAdapter demo\n",
    "- Minimal property-based demonstration for MockLLM\n",
    "\n",
    "All status/instruction text removed; cells are focused on runnable demos and examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a449983",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This consolidated demo notebook is focused on runnable examples that match the code and tests in the repository. Use the demo cells to validate the deterministic mock implementations and renderer output.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
