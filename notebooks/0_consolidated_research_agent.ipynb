{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2d40ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "The following cells will be filled in as each framework feature is implemented:\n",
    "\n",
    "- Models demonstration  \n",
    "- Prompt renderer example  \n",
    "- Agent planning  \n",
    "- Agent run (with MockLLM)  \n",
    "- EvalResults display\n",
    "\n",
    "Each cell will be marked with a comment like `# TASK-3: renderer example` for traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8d2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: Import core framework classes (deferred until implemented)\n",
    "# from research_agent_framework.agents.base import ResearchAgent\n",
    "# from research_agent_framework.llm.client import MockLLM\n",
    "# from research_agent_framework.adapters.search import MockSearchAdapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0c3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: Bootstrap environment\n",
    "# from research_agent_framework.bootstrap import bootstrap\n",
    "# bootstrap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19d1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: sys.path fix for local imports\n",
    "import sys\n",
    "sys.path.append('../src')  # Adjust path as needed for local package imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99578f6",
   "metadata": {},
   "source": [
    "# Consolidated Research Agent Demo\n",
    "\n",
    "This notebook demonstrates the use of the `research_agent_framework` package. Each section will be updated as new features are implemented.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32cd41",
   "metadata": {},
   "source": [
    "# Consolidated Research Agent Demo Notebook\n",
    "\n",
    "This notebook demonstrates the use of the `research_agent_framework` package and its components. Each section is marked for traceability to the corresponding PRD task.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca0d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-7.1: Bootstrap environment\n",
    "from research_agent_framework.bootstrap import bootstrap\n",
    "bootstrap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b916ee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">scope</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Scope</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">topic</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Coffee Shops'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Find coffee shops in SF'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">constraints</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'no paid sources'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mscope\u001b[0m=\u001b[1;35mScope\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtopic\u001b[0m=\u001b[32m'Coffee Shops'\u001b[0m, \u001b[33mdescription\u001b[0m=\u001b[32m'Find coffee shops in SF'\u001b[0m, \u001b[33mconstraints\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'no paid sources'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">task</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResearchTask</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'t-001'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">query</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'best coffee in soma'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">context</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">notes</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mtask\u001b[0m=\u001b[1;35mResearchTask\u001b[0m\u001b[1m(\u001b[0m\u001b[33mid\u001b[0m=\u001b[32m't-001'\u001b[0m, \u001b[33mquery\u001b[0m=\u001b[32m'best coffee in soma'\u001b[0m, \u001b[33mcontext\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mnotes\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">eval_result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EvalResult</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">task_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'t-001'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">success</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95</span>, <span style=\"color: #808000; text-decoration-color: #808000\">feedback</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Looks good'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">details</span>=<span style=\"font-weight: bold\">{})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33meval_result\u001b[0m=\u001b[1;35mEvalResult\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtask_id\u001b[0m=\u001b[32m't-001'\u001b[0m, \u001b[33msuccess\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.95\u001b[0m, \u001b[33mfeedback\u001b[0m=\u001b[32m'Looks good'\u001b[0m, \u001b[33mdetails\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">serp</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SerpResult</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">title</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Cafe Example'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HttpUrl</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'https://example.com/'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">snippet</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Great coffee'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">raw</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mserp\u001b[0m=\u001b[1;35mSerpResult\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtitle\u001b[0m=\u001b[32m'Cafe Example'\u001b[0m, \u001b[33murl\u001b[0m=\u001b[1;35mHttpUrl\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'https://example.com/'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33msnippet\u001b[0m=\u001b[32m'Great coffee'\u001b[0m, \u001b[33mraw\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK-2.3: models demo\n",
    "from research_agent_framework.config import Settings, get_settings\n",
    "from research_agent_framework.models import Scope, ResearchTask, EvalResult, SerpResult\n",
    "from pydantic import TypeAdapter, HttpUrl\n",
    "from assertpy import assert_that\n",
    "\n",
    "# Construct model instances\n",
    "scope = Scope(topic='Coffee Shops', description='Find coffee shops in SF', constraints=['no paid sources'])\n",
    "task = ResearchTask(id='t-001', query='best coffee in soma')\n",
    "eval_result = EvalResult(task_id=task.id, success=True, score=0.95, feedback='Looks good')\n",
    "# Use TypeAdapter to validate/construct an HttpUrl (pydantic v2)\n",
    "url_adapter = TypeAdapter(HttpUrl)\n",
    "validated_url = url_adapter.validate_python('https://example.com')\n",
    "serp = SerpResult(title='Cafe Example', url=validated_url, snippet='Great coffee', raw={'id': 1})\n",
    "\n",
    "# Example asserts using assertpy\n",
    "assert_that(scope.topic).is_equal_to('Coffee Shops')\n",
    "assert_that(scope.constraints).contains('no paid sources')\n",
    "assert_that(task.id).is_equal_to('t-001')\n",
    "assert_that(eval_result.success).is_true()\n",
    "assert_that(serp.url).is_instance_of(HttpUrl)\n",
    "\n",
    "from rich.console import Console\n",
    "from typing import cast\n",
    "console =get_settings().console\n",
    "assert_that(console).is_not_none()\n",
    "console = cast(Console, console)\n",
    "console.print(f\"{scope=}\")\n",
    "console.print(f\"{task=}\")\n",
    "console.print(f\"{eval_result=}\")\n",
    "console.print(f\"{serp=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3ad70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clarify_with_user_instructions.j2 output:\n",
      " These are the messages that have been exchanged so far from the user asking for the report:\n",
      "<Messages>\n",
      "User: What are the best coffee shops in SF?\n",
      "</Messages>\n",
      "\n",
      "Today's date is 2025-09-05.\n",
      "\n",
      "Assess whether you need to ask a clarifying question, or if the user has already provided enough information for you to start research.\n",
      "IMPORTANT: If you can see in the messages history that you have already asked a clarifying question, you almost always do not need to ask another one. Only ask another question if ABSOLUTELY NECESSARY.\n",
      "\n",
      "If there are acronyms, abbreviations, or unknown terms, ask the user to clarify.\n",
      "If you need to ask a question, follow these guidelines:\n",
      "- Be concise while gathering all necessary information\n",
      "- Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner.\n",
      "- Use bullet points or numbered lists if appropriate for clarity. Make sure that this uses markdown formatting and will be rendered correctly if the string output is passed to a markdown renderer.\n",
      "- Don't ask for unnecessary information, or information that the user has already provided. If you can see that the user has already provided the information, do not ask for it again.\n",
      "\n",
      "Respond in valid JSON format with these exact keys:\n",
      "\"need_clarification\": boolean,\n",
      "\"question\": \"<question to ask the user to clarify the report scope>\",\n",
      "\"verification\": \"<verification message that we will start research>\"\n",
      "\n",
      "If you need to ask a clarifying question, return:\n",
      "\"need_clarification\": true,\n",
      "\"question\": \"<your clarifying question>\",\n",
      "\"verification\": \"\"\n",
      "\n",
      "If you do not need to ask a clarifying question, return:\n",
      "\"need_clarification\": false,\n",
      "\"question\": \"\",\n",
      "\"verification\": \"<acknowledgement message that you will now start research based on the provided information>\"\n",
      "\n",
      "For the verification message when no clarification is needed:\n",
      "- Acknowledge that you have sufficient information to proceed\n",
      "- Briefly summarize the key aspects of what you understand from their request\n",
      "- Confirm that you will now begin the research process\n",
      "- Keep the message concise and professional\n",
      "research_agent_prompt.j2 output:\n",
      " You are a research assistant conducting research on the user's input topic. For context, today's date is 2025-09-05.\n",
      "\n",
      "<Task>\n",
      "Your job is to use tools to gather information about the user's input topic.\n",
      "You can use any of the tools provided to you to find resources that can help answer the research question. You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
      "</Task>\n",
      "\n",
      "<Available Tools>\n",
      "You have access to two main tools:\n",
      "1. **tavily_search**: For conducting web searches to gather information\n",
      "2. **think_tool**: For reflection and strategic planning during research\n",
      "\n",
      "**CRITICAL: Use think_tool after each search to reflect on results and plan next steps**\n",
      "</Available Tools>\n",
      "\n",
      "<Instructions>\n",
      "Think like a human researcher with limited time. Follow these steps:\n",
      "\n",
      "1. **Read the question carefully** - What specific information does the user need?\n",
      "2. **Start with broader searches** - Use broad, comprehensive queries first\n",
      "3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
      "4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
      "5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
      "</Instructions>\n",
      "\n",
      "<Hard Limits>\n",
      "**Tool Call Budgets** (Prevent excessive searching):\n",
      "- **Simple queries**: Use 2-3 search tool calls maximum\n",
      "- **Complex queries**: Use up to 5 search tool calls maximum\n",
      "- **Always stop**: After 5 search tool calls if you cannot find the right sources\n",
      "\n",
      "**Stop Immediately When**:\n",
      "- You can answer the user's question comprehensively\n",
      "- You have 3+ relevant examples/sources for the question\n",
      "- Your last 2 searches returned similar information\n",
      "</Hard Limits>\n",
      "\n",
      "<Show Your Thinking>\n",
      "After each search tool call, use think_tool to analyze the results:\n",
      "- What key information did I find?\n",
      "- What's missing?\n",
      "- Do I have enough to answer the question comprehensively?\n",
      "- Should I search more or provide my answer?\n"
     ]
    }
   ],
   "source": [
    "# TASK-3: renderer example\n",
    "from research_agent_framework.prompts import renderer\n",
    "\n",
    "# Render clarify_with_user_instructions template\n",
    "clarify_context = {\"messages\": \"User: What are the best coffee shops in SF?\", \"date\": \"2025-09-05\"}\n",
    "clarify_rendered = renderer.render_template(\"clarify_with_user_instructions.j2\", clarify_context)\n",
    "print(\"clarify_with_user_instructions.j2 output:\\n\", clarify_rendered)\n",
    "\n",
    "# Render research_agent_prompt template\n",
    "agent_context = {\"date\": \"2025-09-05\"}\n",
    "agent_rendered = renderer.render_template(\"research_agent_prompt.j2\", agent_context)\n",
    "print(\"research_agent_prompt.j2 output:\\n\", agent_rendered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d394de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockLLM output: mock response for: What are the best coffee shops in SF?\n"
     ]
    }
   ],
   "source": [
    "# TASK-4.3: Import and use MockLLM for deterministic LLM output demo\n",
    "from research_agent_framework.llm.client import MockLLM, LLMConfig\n",
    "mock_config = LLMConfig(api_key=\"test\", model=\"mock-model\")\n",
    "mock_llm = MockLLM(mock_config)\n",
    "import asyncio\n",
    "async def demo_llm():\n",
    "    result = await mock_llm.generate(\"What are the best coffee shops in SF?\")\n",
    "    print(\"MockLLM output:\", result)\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    import nest_asyncio; nest_asyncio.apply()\n",
    "    asyncio.run(demo_llm())\n",
    "except RuntimeError:\n",
    "    asyncio.run(demo_llm())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675b8b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockLLM property-based output: mock response for: Show me the best coffee shops in SF\n"
     ]
    }
   ],
   "source": [
    "# TASK-4A.3: Property-based tests for LLM client (MockLLM) demo\n",
    "from research_agent_framework.llm.client import LLMConfig, MockLLM\n",
    "from hypothesis import given, strategies as st\n",
    "import asyncio\n",
    "import pytest\n",
    "from assertpy import assert_that\n",
    "\n",
    "# Example: deterministic output for random prompt/config\n",
    "@pytest.mark.asyncio\n",
    "@given(\n",
    "    prompt=st.text(min_size=1, max_size=200),\n",
    "    api_key=st.text(min_size=1, max_size=20),\n",
    "    model=st.text(min_size=1, max_size=20)\n",
    "    )\n",
    "async def demo_mockllm_property_valid(prompt, api_key, model):\n",
    "    config = LLMConfig(api_key=api_key, model=model)\n",
    "    client = MockLLM(config)\n",
    "    result = await client.generate(prompt)\n",
    "    assert_that(result).is_equal_to(f\"mock response for: {prompt}\")\n",
    "\n",
    "# Run a single example for demonstration\n",
    "async def run_demo():\n",
    "    config = LLMConfig(api_key=\"demo-key\", model=\"demo-model\")\n",
    "    client = MockLLM(config)\n",
    "    result = await client.generate(\"Show me the best coffee shops in SF\")\n",
    "    print(\"MockLLM property-based output:\", result)\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    import nest_asyncio; nest_asyncio.apply()\n",
    "    asyncio.run(run_demo())\n",
    "except RuntimeError:\n",
    "    asyncio.run(run_demo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f343e",
   "metadata": {},
   "source": [
    "# TASK-4A.4: Notebook and test documentation updates\n",
    "\n",
    "All property-based tests for models, prompt renderer, and LLM client are now implemented and passing.\n",
    "\n",
    "- Tests are grouped by class for clarity and maintainability.\n",
    "- Each notebook cell demonstrates a key feature or test, traceable to the PRD task.\n",
    "- See previous cells for live code examples and property-based test demonstrations.\n",
    "\n",
    "Ready to proceed to agent implementation and integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a449983",
   "metadata": {},
   "source": [
    "---\n",
    "**Test Coverage Update (2025-09-05):**\n",
    "\n",
    "- All major components (config, bootstrap, logging, renderer, models, LLM client) now have >95% coverage.\n",
    "- Property-based and edge-case tests are grouped by class and cover all error branches.\n",
    "- All tests pass (43/43).\n",
    "- See `tasks/tasks-prd-research-agent-framework.md` for detailed mapping and test report.\n",
    "\n",
    "*This notebook reflects the latest coverage improvements and test results for the research agent framework.*\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
